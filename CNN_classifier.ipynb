{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu0mRG_kn3_m",
        "outputId": "6d45e3f8-9169-490b-acf8-2394e9bd6170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "_mBoF1Cdmgyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation\n",
        "def dataset_validation(loader, model):\n",
        "  class_labels = []\n",
        "  class_preds = []\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      for images, labels in loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          # max returns (value ,index)\n",
        "          values, predicted = torch.max(outputs.data, 1)\n",
        "          n_samples += labels.size(0)\n",
        "          n_correct += (predicted == labels[:,1]).sum().item()\n",
        "\n",
        "      acc = 100.0 * n_correct / n_samples\n",
        "  return acc\n",
        "\n",
        "#Generate new_filename\n",
        "def generate_folder_name(Name_NN, directory):\n",
        "    count = 1\n",
        "    FolderName = Name_NN\n",
        "\n",
        "    while os.path.exists(os.path.join(directory, FolderName)):\n",
        "        # If file exist append a number to the name\n",
        "        FolderName = f\"{Name_NN}_{count}\"\n",
        "        count += 1\n",
        "\n",
        "    return FolderName"
      ],
      "metadata": {
        "id": "JmH17hi2lNxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################GUARDADO##############################################\n",
        "#RUTA GENERAL DE GUARDADO:\n",
        "#path_NN='/content/drive/MyDrive/Colab Notebooks/ARCHIVO_FINAL_PROYECTO_RL/NN/'\n",
        "path_TENSORBOARD = '/content/drive/MyDrive/Colab Notebooks/PROYECT_EMOTION/tensorboard_emot'\n",
        "#Ruta de guardado\n",
        "Name_NN=\"CNN\"\n",
        "FolderName = generate_folder_name(Name_NN, path_TENSORBOARD)\n",
        "print(FolderName)\n",
        "##########################################################################################\n",
        "\n",
        "\n",
        "#Tensorboard\n",
        "writer = SummaryWriter('/content/drive/MyDrive/Colab Notebooks/PROYECT_EMOTION/tensorboard_emot/' + FolderName)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/Colab\\ Notebooks/PROYECT_EMOTION/tensorboard_emot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XIqibH7vgGq",
        "outputId": "835ba4a1-5437-4ce0-b411-86df50b8e9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN_9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncM_bCG2oLOk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.x[idx], self.y[idx]\n",
        "        return sample\n",
        "\n",
        "train_dataset = CustomDataset()\n",
        "test_dataset = CustomDataset()\n",
        "\n",
        "#Dataset\n",
        "path_data = '/content/drive/MyDrive/Colab Notebooks/PROYECT_EMOTION/data_norm/samples.npy'\n",
        "path_labels = '/content/drive/MyDrive/Colab Notebooks/PROYECT_EMOTION/data_norm/valence_labels_bce.npy'\n",
        "train_dataset.x = torch.from_numpy(np.load(path_data).astype(np.float32))\n",
        "train_dataset.y = torch.from_numpy(np.load(path_labels).astype(np.float32) )\n",
        "\n",
        "train_dataset.x, test_dataset.x, train_dataset.y, test_dataset.y = train_test_split(\n",
        "    train_dataset.x, train_dataset.y, test_size=0.1, random_state = True, stratify = train_dataset.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-oSfDa4sLfc",
        "outputId": "cf6a1be8-8a2e-47f4-80f7-9bb597f039f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sample shape: torch.Size([24192, 1, 384, 32])\n",
            "Test sample shape: torch.Size([2688, 1, 384, 32])\n",
            "Train sample type: torch.float32\n",
            "Test sample type: torch.float32\n",
            "Train labels shape: torch.Size([24192, 2])\n",
            "Test labels shape: torch.Size([2688, 2])\n",
            "Train labels type: torch.float32\n",
            "Test labels type: torch.float32\n",
            "cantidad ceros: 13873 cantidad de 1: 10319\n",
            "cantidad ceros: 1541 cantidad de 1: 1147\n"
          ]
        }
      ],
      "source": [
        "print('Train sample shape:', train_dataset.x.size())\n",
        "print('Test sample shape:', test_dataset.x.size())\n",
        "print('Train sample type:', train_dataset.x.dtype)\n",
        "print('Test sample type:', test_dataset.x.dtype)\n",
        "\n",
        "print('Train labels shape:', train_dataset.y.size())\n",
        "print('Test labels shape:', test_dataset.y.size())\n",
        "print('Train labels type:', train_dataset.y.dtype)\n",
        "print('Test labels type:', test_dataset.y.dtype)\n",
        "\n",
        "\n",
        "count_1 = 0\n",
        "count_0 = 0\n",
        "len_train = train_dataset.__len__()\n",
        "for i in range(len_train):\n",
        "  if(train_dataset.y[i, 0] == 1):\n",
        "    count_0+=1\n",
        "  else:\n",
        "    count_1+=1\n",
        "\n",
        "print('cantidad ceros:', count_0, 'cantidad de 1:', count_1)\n",
        "\n",
        "count_1 = 0\n",
        "count_0 = 0\n",
        "len_train = test_dataset.__len__()\n",
        "for i in range(len_train):\n",
        "  if(test_dataset.y[i, 0] == 1):\n",
        "    count_0+=1\n",
        "  else:\n",
        "    count_1+=1\n",
        "\n",
        "print('cantidad ceros:', count_0, 'cantidad de 1:', count_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv2LBKNIyR-i"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "Momentum = 0.9\n",
        "Dropout = 0.25\n",
        "\n",
        "#DataLoader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn9AJuvs5VjW"
      },
      "outputs": [],
      "source": [
        "#Creation of the CNN\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        # Layer 1: Input\n",
        "        self.conv1 = nn.Conv2d(1, 25, kernel_size=(5, 1))\n",
        "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
        "        # Layer 2: conv_2\n",
        "        self.conv2 = nn.Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 2))\n",
        "        self.bn1 = nn.BatchNorm2d(25, momentum = 0.9)\n",
        "        self.pool1 = nn.MaxPool2d((2, 1))\n",
        "        # Layer 3: conv_3\n",
        "        self.conv3 = nn.Conv2d(25, 50, kernel_size=(5, 1))\n",
        "        self.dropout2 = nn.Dropout2d(p=0.25)\n",
        "        # Layer 4: conv_4\n",
        "        self.conv4 = nn.Conv2d(50, 50, kernel_size=(1, 3), stride=(1, 2))\n",
        "        self.bn2 = nn.BatchNorm2d(50, momentum = 0.9)\n",
        "        self.pool2 = nn.MaxPool2d((2, 1))\n",
        "        # Layer 5: conv_5\n",
        "        self.conv5 = nn.Conv2d(50, 100, kernel_size=(5, 1))\n",
        "        self.dropout3 = nn.Dropout2d(p=0.25)\n",
        "        # Layer 6: conv_6\n",
        "        self.conv6 = nn.Conv2d(100, 100, kernel_size=(1, 3), stride=(1, 2))\n",
        "        self.bn3 = nn.BatchNorm2d(100, momentum = 0.9)\n",
        "        self.pool3 = nn.MaxPool2d((2, 1))\n",
        "        # Layer 7: conv_7\n",
        "        self.conv7 = nn.Conv2d(100, 200, kernel_size=(5, 1))\n",
        "        self.dropout4 = nn.Dropout2d(p=0.25)\n",
        "        # Layer 8: conv_8\n",
        "        self.conv8 = nn.Conv2d(200, 200, kernel_size=(1, 3))\n",
        "        self.bn4 = nn.BatchNorm2d(200, momentum = 0.9)\n",
        "        # Flatten layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Linear layers\n",
        "        self.fc1 = nn.Linear( 8000, 256)\n",
        "        self.dropout5 = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(256, 2)  # Output for binary classification task\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1: Input\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.3) #negative slope equals to alpha\n",
        "        x = self.dropout1(x)\n",
        "        # Layer 2: conv_2\n",
        "        x = F.leaky_relu(self.conv2(x), negative_slope=0.3)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool1(x)\n",
        "        # Layer 3: conv_3\n",
        "        x = F.leaky_relu(self.conv3(x), negative_slope=0.3)\n",
        "        x = self.dropout2(x)\n",
        "        # Layer 4: conv_4\n",
        "        x = F.leaky_relu(self.conv4(x), negative_slope=0.3)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool2(x)\n",
        "        # Layer 5: conv_5\n",
        "        x = F.leaky_relu(self.conv5(x), negative_slope=0.3)\n",
        "        x = self.dropout3(x)\n",
        "        # Layer 6: conv_6\n",
        "        x = F.leaky_relu(self.conv6(x), negative_slope=0.3)\n",
        "        x = self.bn3(x)\n",
        "        x = self.pool3(x)\n",
        "        # Layer 7: conv_7\n",
        "        x = F.leaky_relu(self.conv7(x), negative_slope=0.3)\n",
        "        x = self.dropout4(x)\n",
        "        # Layer 8: conv_8\n",
        "        x = F.leaky_relu(self.conv8(x), negative_slope=0.3)\n",
        "        x = self.bn4(x)\n",
        "        # Flatten\n",
        "        x = self.flatten(x)\n",
        "        # Linear layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.dropout5(x)\n",
        "        x = self.fc2(x)\n",
        "        #Normalization with softmax layer\n",
        "        x = F.softmax(x, dim = 1)\n",
        "\n",
        "        return x\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "#Run model on the GPU\n",
        "model = CustomCNN().to(device)\n",
        "\n",
        "examples = iter(test_loader)\n",
        "example_data, example_targets = next(examples)\n",
        "\n",
        "\n",
        "\n",
        "writer.add_graph(model, example_data.to(device))\n",
        "writer.close()\n",
        "#sys.exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "Pw8blqGlg2bh",
        "outputId": "c9e3b6c7-cd34-4a3b-a8a0-d7feda408f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Step [79/189], Loss: 0.7181\n",
            "Epoch [1/200], Step [158/189], Loss: 0.7306\n",
            "Epoch [1],  Accuracy on the test dataset: 55.410879629629626, Accuracy on the test dataset: 53.236607142857146\n",
            "Epoch [2/200], Step [79/189], Loss: 0.6612\n",
            "Epoch [2/200], Step [158/189], Loss: 0.6827\n",
            "Epoch [2],  Accuracy on the test dataset: 56.73776455026455, Accuracy on the test dataset: 54.055059523809526\n",
            "Epoch [3/200], Step [79/189], Loss: 0.6937\n",
            "Epoch [3/200], Step [158/189], Loss: 0.6722\n",
            "Epoch [3],  Accuracy on the test dataset: 57.30820105820106, Accuracy on the test dataset: 56.54761904761905\n",
            "Epoch [4/200], Step [79/189], Loss: 0.7198\n",
            "Epoch [4/200], Step [158/189], Loss: 0.6575\n",
            "Epoch [4],  Accuracy on the test dataset: 58.42427248677249, Accuracy on the test dataset: 57.961309523809526\n",
            "Epoch [5/200], Step [79/189], Loss: 0.6850\n",
            "Epoch [5/200], Step [158/189], Loss: 0.6578\n",
            "Epoch [5],  Accuracy on the test dataset: 59.664351851851855, Accuracy on the test dataset: 57.03125\n",
            "Epoch [6/200], Step [79/189], Loss: 0.6894\n",
            "Epoch [6/200], Step [158/189], Loss: 0.6802\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-519e988189bc>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch+1}],  Accuracy on the test dataset: {train_acc}, Accuracy on the test dataset: {test_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-417a466a7340>\u001b[0m in \u001b[0;36mdataset_validation\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mn_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#Model evaluation\n",
        "loss_evol = []\n",
        "running_loss = 0\n",
        "running_correct = 0\n",
        "\n",
        "#Print/iterations\n",
        "iter_print = 79\n",
        "\n",
        "n_total_steps = len(train_loader) #sample number / batch size --> 41948/128\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (signals, labels) in enumerate(train_loader):\n",
        "        #signals.reshape(128, 1, 32, 384)\n",
        "        signals = signals.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(signals)\n",
        "        #loss = criterion(m(outputs), labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Display\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        running_correct += (predicted == labels[:, 1]).sum().item()\n",
        "\n",
        "        if (i+1) % iter_print == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "            #Tensorboard\n",
        "            writer.add_scalar('training loss', running_loss / iter_print, epoch * n_total_steps + i)\n",
        "            running_accuracy = running_correct / iter_print / predicted.size(0)\n",
        "            writer.add_scalar('accuracy train dataset', running_accuracy, epoch * n_total_steps + i)\n",
        "            running_correct = 0\n",
        "            running_loss = 0.0\n",
        "\n",
        "    train_acc = dataset_validation(train_loader, model)\n",
        "    test_acc = dataset_validation(test_loader, model)\n",
        "    print (f'Epoch [{epoch+1}],  Accuracy on the test dataset: {train_acc}, Accuracy on the test dataset: {test_acc}')\n",
        "\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = path_TENSORBOARD + FolderName\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "class_labels = []\n",
        "class_preds = []\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        values, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
        "\n",
        "        class_preds.append(class_probs_batch)\n",
        "        class_labels.append(labels)\n",
        "\n",
        "    # 10000, 10, and 10000, 1\n",
        "    # stack concatenates tensors along a new dimension\n",
        "    # cat concatenates tensors in the given dimension\n",
        "    class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n",
        "    class_labels = torch.cat(class_labels)\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the test images: {acc} %')\n",
        "\n",
        "    ############## TENSORBOARD ########################\n",
        "    classes = range(2)\n",
        "    for i in classes:\n",
        "        labels_i = class_labels == i\n",
        "        preds_i = class_preds[:, i]\n",
        "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
        "        writer.close()\n",
        "    ###################################################\n"
      ],
      "metadata": {
        "id": "5zCy43pkq1Va"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}